{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bKDIpgg0eZ2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc3a8a6-b974-44a0-91cf-176904f42b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # used for convergence criteria\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "i1jRZ8tOeyk9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = \"/content/drive/My Drive/Dataset/HIGGS_train.csv\"  # path to csv\n",
        "\n",
        "# Defining column names, Loading data\n",
        "column_names = [\"outcome\"] + [\"feature \"+str(i) for i in range(1,29)]\n",
        "df = pd.read_csv(csv_file, header=None, names=column_names)\n",
        "\n",
        "# converting strings to float and removing rows with nan values. (pre-processing)\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "uM5auSsOe1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e2f254-b4e4-49c5-bdd4-11cbfdd8813a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-5184d83a31cf>:5: DtypeWarning: Columns (8,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(csv_file, header=None, names=column_names)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df, test_df = np.split(df.sample(frac=1, random_state=42), [int(0.6*len(df)), int(0.8*len(df))]) #splitting the data"
      ],
      "metadata": {
        "id": "WR4Bt4sBe9eq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below is used to create a tensorflow dataframe with the training data. Prior to using this dataframe, the RAM would constantly overflow. With this dataframe, however,\n",
        "# the RAM is far from overflowing. \n",
        "\n",
        "\n",
        "train_df, val_df, test_df = np.split(df.sample(frac=1, random_state=42), [int(0.6*len(df)), int(0.8*len(df))]) #splitting the data\n",
        "\n",
        "# Creating tensors from the previous partitions, which will be used below to create the tensorflow dataset.\n",
        "train_labels = tf.constant(train_df[\"outcome\"].values)\n",
        "train_features = tf.constant(train_df.drop(\"outcome\", axis=1).values)\n",
        "val_labels = tf.constant(val_df[\"outcome\"].values)\n",
        "val_features = tf.constant(val_df.drop(\"outcome\", axis=1).values)\n",
        "test_labels = tf.constant(test_df[\"outcome\"].values)\n",
        "test_features = tf.constant(test_df.drop(\"outcome\", axis=1).values)\n",
        "\n",
        "# Create TensorFlow datasets from the data (features are 1 column, train labels are the other column)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
        "\n",
        "#shuffling and batching the data.\n",
        "batch_size = 32\n",
        "train_buffer_size = tf.data.experimental.cardinality(train_dataset).numpy()   # setting buffer of the shuffle function to the maximum number of elements in the respective dataframe\n",
        "val_buffer_size = tf.data.experimental.cardinality(val_dataset).numpy()\n",
        "test_buffer_size = tf.data.experimental.cardinality(test_dataset).numpy()\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size=train_buffer_size)\n",
        "train_dataset = train_dataset.batch(batch_size=batch_size)  # in this dataset, every element is a batch, which makes the code memory efficient\n",
        "\n",
        "val_dataset = val_dataset.shuffle(buffer_size=val_buffer_size)\n",
        "val_dataset = val_dataset.batch(batch_size=batch_size)\n",
        "\n",
        "test_dataset = test_dataset.shuffle(buffer_size=test_buffer_size)\n",
        "test_dataset = test_dataset.batch(batch_size=batch_size)\n",
        "\n",
        "# prefetching the data that will be used after the current one. \n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  #tf.data.AUTOTUNE allows tensorflow to automatically determine the right amount of buffer size for prefetch\n",
        "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "6DAGOXZLfAin"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_elements = tf.data.experimental.cardinality(train_dataset).numpy()"
      ],
      "metadata": {
        "id": "Cn1CTrKJfGlY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of batches in the tensor\n",
        "num_datapoints = len(train_dataset)\n",
        "print(num_datapoints)"
      ],
      "metadata": {
        "id": "bm5YfIq5fag1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc95be7-2b7d-4885-cca7-cedf1c5bb99c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first model\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "output_model1 = model_1.fit(train_dataset, validation_data=val_dataset, epochs=2,callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "V1YxW0V-fbIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852993c3-e3cc-4edb-8d36-e741e63c5087"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "11250/11250 [==============================] - 21s 2ms/step - loss: 0.6134 - accuracy: 0.6568 - val_loss: 0.5850 - val_accuracy: 0.6870\n",
            "Epoch 2/2\n",
            "11250/11250 [==============================] - 21s 2ms/step - loss: 0.5811 - accuracy: 0.6898 - val_loss: 0.5692 - val_accuracy: 0.7012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing for accuracy \n",
        "loss, accuracy = model_1.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSae3iL6gHNR",
        "outputId": "8e14a433-8c20-4198-c397-fff28e5bc308"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3750/3750 [==============================] - 4s 951us/step - loss: 0.5689 - accuracy: 0.7003\n",
            "Test loss: 0.5688731074333191\n",
            "Test accuracy: 0.7002750039100647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TjLnuxCjgSws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}